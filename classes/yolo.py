#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2024/2/17  18:36
# @Author  : è èå¹é›ª
# @Software: PyCharm
# @Describe: 
# -*- encoding:utf-8 -*-
import threading
import time
from collections import defaultdict
from pathlib import Path

import PIL
import cv2
import numpy as np
import torch
from PIL.Image import Image
from PySide6.QtCore import QObject
from ultralytics import YOLO
from ultralytics.yolo.cfg import get_cfg
from ultralytics.yolo.data.dataloaders.stream_loaders import LoadStreams
from ultralytics.yolo.engine.predictor import BasePredictor
from ultralytics.yolo.engine.results import Results
from ultralytics.yolo.utils import DEFAULT_CFG, LOGGER, SETTINGS, callbacks, ops
from ultralytics.yolo.utils.checks import check_imshow
from ultralytics.yolo.utils.files import increment_path
from ultralytics.yolo.utils.plotting import Annotator, colors, save_one_box
from ultralytics.yolo.utils.torch_utils import smart_inference_mode
from ui.utils.UIFunctions import *
import datetime
import supervision as sv


class YoloPredictor(BasePredictor, QObject):
    # ä¿¡å·å®šä¹‰
    yolo2main_pre_img = Signal(np.ndarray)  # åŸå§‹å›¾åƒä¿¡å·
    yolo2main_res_img = Signal(np.ndarray)  # æµ‹è¯•ç»“æœä¿¡å·
    yolo2main_status_msg = Signal(str)  # æ£€æµ‹/æš‚åœ/åœæ­¢/æµ‹è¯•å®Œæˆ/é”™è¯¯æŠ¥å‘Šä¿¡å·
    yolo2main_fps = Signal(str)  # å¸§ç‡ä¿¡å·
    yolo2main_labels = Signal(dict)  # æ£€æµ‹ç›®æ ‡ç»“æœï¼ˆæ¯ä¸ªç±»åˆ«çš„æ•°é‡ï¼‰
    yolo2main_progress = Signal(int)  # è¿›åº¦ä¿¡å·
    yolo2main_class_num = Signal(int)  # æ£€æµ‹åˆ°çš„ç±»åˆ«æ•°
    yolo2main_target_num = Signal(int)  # æ£€æµ‹åˆ°çš„ç›®æ ‡æ•°
    yolo2main_tts_dialog = Signal(int)  # ç«æºè­¦å‘Š
    yolo2main_tts = Signal(int)

    def __init__(self, cfg=DEFAULT_CFG, overrides=None):
        super(YoloPredictor, self).__init__()
        QObject.__init__(self)

        # è·å–é…ç½®
        self.args = get_cfg(cfg, overrides)
        project = self.args.project or Path(SETTINGS['runs_dir']) / self.args.task
        name = f'{self.args.mode}'
        self.save_dir = increment_path(Path(project) / name, exist_ok=self.args.exist_ok)
        self.done_warmup = False
        if self.args.show:
            self.args.show = check_imshow(warn=True)

        # GUI å‚æ•°
        self.used_model_name = None  # ä½¿ç”¨çš„æ£€æµ‹æ¨¡å‹åç§°
        self.new_model_name = None  # å®æ—¶æ›´æ”¹çš„æ¨¡å‹
        self.source = ''  # è¾“å…¥æº
        self.stop_dtc = False  # ç»ˆæ­¢æ£€æµ‹
        self.continue_dtc = True  # æš‚åœæ£€æµ‹
        self.save_res = False  # ä¿å­˜æµ‹è¯•ç»“æœ
        self.save_txt = False  # ä¿å­˜æ ‡ç­¾(txt)æ–‡ä»¶
        self.iou_thres = 0.45  # IoU é˜ˆå€¼
        self.conf_thres = 0.25  # ç½®ä¿¡åº¦é˜ˆå€¼
        self.speed_thres = 10  # å»¶è¿Ÿï¼Œæ¯«ç§’
        self.labels_dict = {}  # è¿”å›ç»“æœçš„å­—å…¸
        self.progress_value = 0  # è¿›åº¦æ¡

        # ä»…åœ¨é…ç½®è®¾ç½®å®Œæˆåå¯ç”¨
        self.model = None
        self.data = self.args.data  # data_dict
        self.imgsz = None
        self.device = None
        self.dataset = None
        self.vid_path, self.vid_writer = None, None
        self.annotator = None
        self.data_path = None
        self.source_type = None
        self.batch = None
        self.callbacks = defaultdict(list, callbacks.default_callbacks)  # æ·»åŠ å›è°ƒå‡½æ•°
        callbacks.add_integration_callbacks(self)

        self.fire_flag = False  # æ˜¯å¦è¯†åˆ«åˆ°ç«ç„°
        self.isFirstTTS = True  # æ˜¯å¦ä¸ºç¬¬ä¸€æ¬¡è¯†åˆ«åˆ°ç«ç„°

        # è¿è¡Œæ—¶å€™çš„å‚æ•°æ”¾è¿™é‡Œ
        self.start_time = 0  # æ‹¿æ¥ç®—FPSçš„è®¡æ•°å˜é‡
        self.count = 0
        self.sum_of_count = 0
        self.class_num = 0
        self.total_frames = 0
        self.lock_id = 0

        # è®¾ç½®çº¿æ¡æ ·å¼    åšåº¦ & ç¼©æ”¾å¤§å°
        self.box_annotator = sv.BoxAnnotator(
            thickness=2,
            text_thickness=1,
            text_scale=0.5
        )

    # ä¸»è¦æ£€æµ‹æ–¹æ³•
    @smart_inference_mode()
    def run(self):
        # try:
            if self.args.verbose:
                LOGGER.info('')

            # è®¾ç½®æ¨¡å‹
            self.yolo2main_status_msg.emit('åŠ è½½æ¨¡å‹...')
            if not self.model:
                self.setup_model(self.new_model_name)
                self.used_model_name = self.new_model_name

            # è®¾ç½®è¾“å…¥æº
            print('è¾“å…¥æºï¼š' + str(self.source) + '\n')
            self.setup_source(self.source if self.source is not None else self.args.source)
            model = YOLO(self.new_model_name)

            # ä½¿ç”¨OpenCVè¯»å–è§†é¢‘â€”â€”è·å–è¿›åº¦æ¡???
            if 'mp4' in self.source or 'avi' in self.source or 'mkv' in self.source or 'flv' in self.source or 'mov' in self.source:
                cap = cv2.VideoCapture(self.source)
                self.total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)
                cap.release()

            # æ£€æŸ¥ä¿å­˜è·¯å¾„/æ ‡ç­¾
            if self.save_res or self.save_txt:
                (self.save_dir / 'labels' if self.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)

            # è·å–æ•°æ®æº ï¼ˆä¸åŒçš„ç±»å‹è·å–ä¸åŒçš„æ•°æ®æºï¼‰
            iter_model = iter(
                model.track(source=self.source, show=False, stream=True, iou=self.iou_thres, conf=self.conf_thres))

            # é¢„çƒ­æ¨¡å‹
            if not self.done_warmup:
                print('é¢„çƒ­æ¨¡å‹ï¼')
                self.model.warmup(imgsz=(1 if self.model.pt or self.model.triton else self.dataset.bs, 3, *self.imgsz))
                self.done_warmup = True

            self.seen, self.windows, self.dt, self.batch = 0, [], (ops.Profile(), ops.Profile(), ops.Profile()), None

            # å¼€å§‹æ£€æµ‹
            print('å¼€å§‹æ£€æµ‹ï¼')
            count = 0  # è¿è¡Œå®šä½å¸§
            start_time = time.time()  # ç”¨äºè®¡ç®—å¸§ç‡
            batch = iter(self.dataset)

            while True:
                # ç»ˆæ­¢æ£€æµ‹
                if self.stop_dtc:
                    if isinstance(self.vid_writer[-1], cv2.VideoWriter):
                        self.vid_writer[-1].release()  # é‡Šæ”¾æœ€ç»ˆè§†é¢‘å†™å…¥å™¨
                    self.yolo2main_status_msg.emit('æ£€æµ‹ç»ˆæ­¢!')
                    self.release_capture()
                    break

                # # åœ¨æ£€æµ‹è¿‡ç¨‹ä¸­æ›´æ”¹æ¨¡å‹
                # if self.used_model_name != self.new_model_name:
                #     # self.yolo2main_status_msg.emit('Change Model...')
                #     self.setup_model(self.new_model_name)
                #     self.used_model_name = self.new_model_name

                # æš‚åœå¼€å…³
                if self.continue_dtc:
                    img_res, result, height, width = self.recognize_res(iter_model)
                    self.res_address(img_res, result, model)

                # æ£€æµ‹å®Œæˆ
                # if count + 1 >= all_count:
                #     if isinstance(self.vid_writer[-1], cv2.VideoWriter):
                #         self.vid_writer[-1].release()  # é‡Šæ”¾æœ€ç»ˆè§†é¢‘å†™å…¥å™¨
                #     self.yolo2main_status_msg.emit('Detection completed')
                #     break

        # except Exception as e:
        #     pass
        #     print(e)
        #     self.yolo2main_status_msg.emit('%s' % e)

    # è¿›è¡Œè¯†åˆ«â€”â€”å¹¶è¿”å›æ‰€æœ‰ç»“æœ
    def res_address(self, img_res, result, model):  # self, img_res, result, height, width, model, out
        # å¤åˆ¶ä¸€ä»½
        img_box = np.copy(img_res)  # å³è¾¹çš„å›¾ï¼ˆä¼šç»˜åˆ¶æ ‡ç­¾ï¼ï¼‰ img_resæ˜¯åŸå›¾-ä¸ä¼šå—å½±å“
        img_trail = np.copy(img_res)  # å·¦è¾¹çš„å›¾

        # å¦‚æœæ²¡æœ‰è¯†åˆ«çš„ï¼š
        if result.boxes.id is None:
            # ç›®æ ‡éƒ½æ˜¯0
            self.sum_of_count = 0
            self.class_num = 0
            labels_write = "æš‚æœªè¯†åˆ«åˆ°ç›®æ ‡ï¼"
        # å¦‚æœæœ‰è¯†åˆ«çš„
        else:
            detections = sv.Detections.from_yolov8(result)
            detections.tracker_id = result.boxes.id.cpu().numpy().astype(int)

            # id ã€ä½ç½®ã€ç›®æ ‡æ€»æ•°
            self.labels_dict = {}
            self.class_num = self.get_class_number()  # ç±»åˆ«æ•°
            id = detections.tracker_id  # id
            xyxy = detections.xyxy  # ä½ç½®
            self.sum_of_count = len(id)  # ç›®æ ‡æ€»æ•°

            # ç”»æ ‡ç­¾åˆ°å›¾åƒä¸Š
            if self.class_num != 0:
                img_box = self.box_annotator.annotate(scene=img_box, detections=detections)
            # labels_write, img_box = self.creat_labels(detections, img_box, model)

        # å†™å…¥txtâ€”â€”å­˜å‚¨labelsé‡Œçš„ä¿¡æ¯
        # if self.save_txt:
        #     with open(f'{self.save_txt_path}/result.txt', 'a') as f:
        #         f.write('å½“å‰æ—¶åˆ»å±å¹•ä¿¡æ¯:' +
        #                 str(labels_write) +
        #                 f'æ£€æµ‹æ—¶é—´: {datetime.datetime.now().strftime("%Y-%m-%d-%H:%M:%S")}' +
        #                 f' è·¯æ®µé€šè¿‡çš„ç›®æ ‡æ€»æ•°: {self.sum_of_count}')
        #         f.write('\n')

        # é¢„æµ‹è§†é¢‘å†™å…¥æœ¬åœ° 111
        # if self.save_res:
        #     out.write(img_box)

        # ä¼ é€’ä¿¡å·ç»™ä¸»çª—å£
        self.emit_res(img_trail, img_box)

    # è¯†åˆ«ç»“æœå¤„ç†
    def recognize_res(self, iter_model):
        # æ£€æµ‹ ---ç„¶åè·å–æœ‰ç”¨çš„æ•°æ®
        result = next(iter_model)  # è¿™é‡Œæ˜¯æ£€æµ‹çš„æ ¸å¿ƒï¼Œæ¯æ¬¡å¾ªç¯éƒ½ä¼šæ£€æµ‹ä¸€å¸§å›¾åƒ,å¯ä»¥è‡ªè¡Œæ‰“å°resultçœ‹çœ‹é‡Œé¢æœ‰å“ªäº›keyå¯ä»¥ç”¨
        img_res = result.orig_img  # åŸå›¾
        height, width, _ = img_res.shape

        return img_res, result, height, width

    # ä¿¡å·å‘é€åŒº
    def emit_res(self, img_trail, img_box):
        time.sleep(self.speed_thres / 1000)  # ç¼“å†²
        # è½¨è¿¹å›¾åƒï¼ˆå·¦è¾¹ï¼‰
        self.yolo2main_pre_img.emit(img_trail)
        # æ ‡ç­¾å›¾ï¼ˆå³è¾¹ï¼‰
        self.yolo2main_res_img.emit(img_box)
        # æ€»ç±»åˆ«æ•°é‡ ã€ æ€»ç›®æ ‡æ•°
        self.yolo2main_class_num.emit(self.class_num)
        self.yolo2main_target_num.emit(self.sum_of_count)

        # æŠ¥è­¦
        if self.sum_of_count > 0:
            self.fire_flag = True
            if self.fire_flag & self.isFirstTTS:
                self.isFirstTTS = False
                self.yolo2main_tts_dialog.emit(1)
                self.yolo2main_tts.emit(1)

        # è¿›åº¦æ¡
        if '0' in self.source or 'rtsp' in self.source:
            self.yolo2main_progress.emit(0)
        else:
            self.progress_value = int(self.count / self.total_frames * 1000)
            self.yolo2main_progress.emit(self.progress_value)
        # è®¡ç®—FPS
        self.count += 1
        if self.count % 3 == 0 and self.count >= 3:  # è®¡ç®—FPS
            self.yolo2main_fps.emit(str(int(3 / (time.time() - self.start_time))))
            self.start_time = time.time()

    # def Interface_Refresh(self, im0, im0s, class_nums, target_nums):
    #     # Send test results ã€å‘é€ä¿¡å· ç»™ label æ˜¾ç¤ºå›¾åƒã€‘
    #     self.yolo2main_res_img.emit(im0)  # after detection  ----------ç»“æœ
    #     self.yolo2main_pre_img.emit(im0s if isinstance(im0s, np.ndarray) else im0s[0])  # Before testing
    #     # self.yolo2main_labels.emit(self.labels_dict)        # webcam need to change the def write_results
    #     self.yolo2main_class_num.emit(class_nums)
    #     self.yolo2main_target_num.emit(target_nums)

    def warmup_models(self):
        # çƒ­èº«æ¨¡å‹
        if not self.done_warmup:
            # è°ƒç”¨æ¨¡å‹çš„ warmup å‡½æ•°ï¼Œå…¶ä¸­ imgsz å‚æ•°ä¸ºè¾“å…¥å›¾åƒçš„å¤§å°
            # å¦‚æœæ¨¡å‹ä½¿ç”¨ PyTorchï¼Œimgsz å‚æ•°åº”ä¸º [batch_size, channels, height, width]
            # å¦‚æœæ¨¡å‹ä½¿ç”¨ Tritonï¼Œimgsz å‚æ•°åº”ä¸º [height, width, channels, batch_size]
            self.model.warmup(
                imgsz=(1 if self.model.pt or self.model.triton else self.dataset.bs, 3, *self.imgsz))
            # å°† done_warmup æ ‡è®°ä¸º Trueï¼Œä»¥æ ‡è®°æ¨¡å‹å·²ç»çƒ­èº«è¿‡
            self.done_warmup = True
            print('çƒ­èº«å®Œæ¯•')

    # è·å– Annotator å®ä¾‹
    def get_annotator(self, img):
        return Annotator(img, line_width=self.args.line_thickness, example=str(self.model.names))

    # å›¾åƒé¢„å¤„ç†
    def preprocess(self, img):
        img = torch.from_numpy(img).to(self.model.device)
        img = img.half() if self.model.fp16 else img.float()  # # uint8 è½¬ä¸º fp16/32
        img /= 255  # 0 - 255 è½¬ä¸º 0.0 - 1.0
        return img

    # é¢„æµ‹ç»“æœåå¤„ç†
    def postprocess(self, preds, img, orig_img):
        ### important
        preds = ops.non_max_suppression(preds,
                                        self.conf_thres,
                                        self.iou_thres,
                                        agnostic=self.args.agnostic_nms,
                                        max_det=self.args.max_det,
                                        classes=self.args.classes)

        results = []
        for i, pred in enumerate(preds):
            orig_img = orig_img[i] if isinstance(orig_img, list) else orig_img
            shape = orig_img.shape
            pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], shape).round()
            path, _, _, _, _ = self.batch
            img_path = path[i] if isinstance(path, list) else path
            results.append(Results(orig_img=orig_img, path=img_path, names=self.model.names, boxes=pred))
        return results

    # å†™å…¥æ£€æµ‹ç»“æœ
    def write_results(self, idx, results, batch):
        p, im, im0 = batch
        log_string = ''
        if len(im.shape) == 3:
            im = im[None]  # ä¸ºæ‰¹æ¬¡ç»´åº¦æ‰©å±•
        self.seen += 1
        imc = im0.copy() if self.args.save_crop else im0
        if self.source_type.webcam or self.source_type.from_img:  # batch_size >= 1         # attention
            log_string += f'{idx}: '
            frame = self.dataset.count
        else:
            frame = getattr(self.dataset, 'frame', 0)
        self.data_path = p
        self.txt_path = str(self.save_dir / 'labels' / p.stem) + ('' if self.dataset.mode == 'image' else f'_{frame}')
        # log_string += '%gx%g ' % im.shape[2:]         # !!! don't add img size~
        self.annotator = self.get_annotator(im0)

        det = results[idx].boxes  # TODO: make boxes inherit from tensors

        if len(det) == 0:
            return f'{log_string}(no detections), '  # if no, send this~~

        for c in det.cls.unique():
            n = (det.cls == c).sum()  # detections per class
            log_string += f"{n}~{self.model.names[int(c)]},"  # {'s' * (n > 1)}, "   # don't add 's'
        # now log_string is the classes ğŸ‘†

        # å†™
        for d in reversed(det):
            cls, conf = d.cls.squeeze(), d.conf.squeeze()
            if self.save_txt:  # å†™å…¥æ–‡ä»¶
                line = (cls, *(d.xywhn.view(-1).tolist()), conf) \
                    if self.args.save_conf else (cls, *(d.xywhn.view(-1).tolist()))  # label format
                with open(f'{self.txt_path}.txt', 'a') as f:
                    f.write(('%g ' * len(line)).rstrip() % line + '\n')
            if self.save_res or self.args.save_crop or self.args.show or True:  # Add bbox to image(must)
                c = int(cls)  # integer class
                name = f'id:{int(d.id.item())} {self.model.names[c]}' if d.id is not None else self.model.names[c]
                label = None if self.args.hide_labels else (name if self.args.hide_conf else f'{name} {conf:.2f}')
                self.annotator.box_label(d.xyxy.squeeze(), label, color=colors(c, True))
            if self.args.save_crop:
                save_one_box(d.xyxy,
                             imc,
                             file=self.save_dir / 'crops' / self.model.model.names[c] / f'{self.data_path.stem}.jpg',
                             BGR=True)

        return log_string

        # é‡Šæ”¾æ‘„åƒå¤´
    def release_capture(self):
        LoadStreams.capture = 'release'  # è¿™é‡Œæ˜¯ä¸ºäº†ç»ˆæ­¢ä½¿ç”¨æ‘„åƒå¤´æ£€æµ‹å‡½æ•°çš„çº¿ç¨‹ï¼Œæ”¹äº†yoloæºç 
    # ä¸ç¡®å®šæ˜¯ä¸æ˜¯è¿™ä¸ªåŒ…ï¼ŒæŠ¥é”™è¿›è¡Œæ›´æ¢

    def get_class_number(self):
        # åªè¯†åˆ«ç«ç„°
        return 1
